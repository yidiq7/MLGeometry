{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FIm6mUYtGC5"
   },
   "source": [
    "# MLGeometry guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCemOECptGC6"
   },
   "source": [
    "This introduction demonstrates how to use MLGeometry to:\n",
    "1. Generate a hypersurface.\n",
    "2. Build a bihomogeneous neural network.\n",
    "3. Use the model to compute numerical Calabi-Yau metrics with the embedding method.\n",
    "4. Plot $\\eta$ on a rational curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilHaPYnkEi-S"
   },
   "source": [
    "## Install the package (on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install MLGeometry-JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyFvWKNmtGC7"
   },
   "source": [
    "## Configure imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "doBhWopntGC9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import pickle\n",
    "\n",
    "import MLGeometry as mlg\n",
    "from MLGeometry import bihomoNN as bnn\n",
    "from typing import Sequence, Any, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9F3AKqPtGC_"
   },
   "source": [
    "Import the libraries to plot the $\\eta$ on the rational curve (see the last section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J9LWSTH3tGC_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLGeometry: Precision set to FP64 (x64 enabled).\n"
     ]
    }
   ],
   "source": [
    "mlg.set_precision(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd1zG07TtGDA"
   },
   "source": [
    "## Set a random seed (optional)\n",
    "Some random seed might be bad for numerical calulations. If there are any errors during the training, you may want to try a different seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5Rz0lXmtGDB"
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "rng = jax.random.PRNGKey(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjB84Ln1tGDB"
   },
   "source": [
    "## Define a hypersurface\n",
    "First define a set of coordinates and a function as sympy symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pwhQynDBtGDB"
   },
   "outputs": [],
   "source": [
    "z0, z1, z2, z3, z4 = sp.symbols('z0, z1, z2, z3, z4')\n",
    "Z = [z0,z1,z2,z3,z4]\n",
    "f = z0**5 + z1**5 + z2**5 + z3**5 + z4**5 + 0.0*z0*z1*z2*z3*z4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTm215kftGDC"
   },
   "source": [
    "Then define a hypersurface as a collection of points which solve the equation f = 0, using the `Hypersurface` class in the `mlg.hypersurface` module. The parameter n_pairs is the number of random pairs of points used to form the random lines in $\\mathbf{CP}^{N+1}$. Then we take the intersections of those random lines and the hypersurface. By Bezout's theorem, each line intersects the hypersurface in precisely d points where d is the number of homogeneous coordinates. So the total number of points is d * n_pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jz1Vi4Y2tGDC"
   },
   "outputs": [],
   "source": [
    "n_pairs = 10000\n",
    "HS_train = mlg.hypersurface.Hypersurface(Z, f, n_pairs)\n",
    "HS_test = mlg.hypersurface.Hypersurface(Z, f, n_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE981r2ctGDC"
   },
   "source": [
    "The Hypersurface class will take care of the patchwork automatically. Let's use the `list_patches` function to check the number of points on each patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zyw84dftGDC",
    "outputId": "e8fb2f30-eadf-4129-80c6-33971a79b273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patches: 5\n",
      "Points on patch 1 : 10171\n",
      "Points on patch 2 : 10038\n",
      "Points on patch 3 : 9783\n",
      "Points on patch 4 : 10047\n",
      "Points on patch 5 : 9961\n"
     ]
    }
   ],
   "source": [
    "HS_train.list_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JtfwpmFtGDD"
   },
   "source": [
    "You can also invoke this method on one of the patches to check the distribution on the subpatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fMZAyKutGDD",
    "outputId": "5b432e0a-36e6-4ac5-9dda-4c9447aa656a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patches: 4\n",
      "Points on patch 1 : 2588\n",
      "Points on patch 2 : 2530\n",
      "Points on patch 3 : 2512\n",
      "Points on patch 4 : 2541\n"
     ]
    }
   ],
   "source": [
    "HS_train.patches[0].list_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8p5D9ZWtGDE"
   },
   "source": [
    "The Hypersurface class contains some symbolic and numerical methods as well, which will be introduced elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u1oIOLytGDE"
   },
   "source": [
    "## Training with JAX\n",
    "\n",
    "### Generate datasets\n",
    "The `mlg.dataset.generate_dataset` function converts a hypersurface to a dataset dictionary, which has four componets: the points on the hypersurface, the volume form $\\small \\Omega \\wedge \\bar\\Omega$, the mass reweighting the points distribution and the restriction which restricts the Kähler metric to a subpatch. The restriction contains an extra linear transformation so that points on different affine patches can all be processed in one call. It is also possible to generate a dataset only on one affine patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uGq-mKdDtGDE"
   },
   "outputs": [],
   "source": [
    "train_set = mlg.dataset.generate_dataset(HS_train)\n",
    "test_set = mlg.dataset.generate_dataset(HS_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHP3ExA1tGDG"
   },
   "source": [
    "Shuffle and batch the datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Mga5fdtGDG"
   },
   "source": [
    "Let's look at what is inside a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  points: shape (50000, 5), dtype complex64\n",
      "  Omega_Omegabar: shape (50000,), dtype float32\n",
      "  mass: shape (50000,), dtype float32\n",
      "  restriction: shape (50000, 3, 5), dtype complex64\n"
     ]
    }
   ],
   "source": [
    "for key, val in train_set.items():\n",
    "    print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kVSGnnktGDH"
   },
   "source": [
    "### Build a bihomogeneous neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV_JRrERtGDI"
   },
   "source": [
    "The `mlg.bihomoNN` module provides the necessary layers (e.g. `Bihomogeneous` and `SquareDense` ) to construct the Kähler potential with a bihomogeneous neural network. Here is an example of a two-hidden-layer network (k = 4) with 70 and 100 hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B9mE_YLltGDI"
   },
   "outputs": [],
   "source": [
    "class BihomogeneousNetwork(nn.Module):\n",
    "    layers: List[int]\n",
    "    amp: Any = 1.0\n",
    "    d: int = 5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = bnn.Bihomogeneous(d=self.d)(inputs)\n",
    "        for feat in self.layers:\n",
    "            x = bnn.SquareDense(features=feat)(x)\n",
    "        x = bnn.SquareDense(features=1, activation=None)(x)\n",
    "\n",
    "        return self.amp * jnp.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNetwork(nn.Module):\n",
    "    layers: List[int]\n",
    "    amp: Any = 1.0\n",
    "    activation: Callable = jnp.tanh\n",
    "    d: int = 5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = inputs\n",
    "        x = bnn.Spectral(d=self.d)(inputs)\n",
    "        for feat in self.layers:\n",
    "            x = bnn.Dense(features=feat, activation=self.activation)(x)\n",
    "        x = bnn.Dense(features=1)(x)\n",
    "\n",
    "        #return self.amp * x\n",
    "        return self.amp * jnp.sinh(x)\n",
    "        #return self.amp * jnp.sign(x) * (jnp.exp(jnp.abs(x)) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cXSQuJgCtGDI"
   },
   "outputs": [],
   "source": [
    "K0 = BihomogeneousNetwork(layers=[64, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'trained_models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model_path = os.path.join(output_dir, '64_256_512_1_lbfgs.pkl')\n",
    "    \n",
    "with open(model_path, 'rb') as f_pkl:\n",
    "    params0 = pickle.load(f_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8R-_rO7tGDK"
   },
   "source": [
    "Loop over the batches and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjRWkGZ4tGDQ",
    "outputId": "273e4c93-53d9-4783-84f0-3e940aab0bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_test = 7.39692e-04\n",
      "E_test = 1.04536e-06\n"
     ]
    }
   ],
   "source": [
    "sigma_test = mlg.loss.evaluate_dataset(K0, params0, test_set, mlg.loss.weighted_MAPE, BATCH_SIZE*10)\n",
    "E_test = mlg.loss.evaluate_dataset(K0, params0, test_set, mlg.loss.weighted_MSPE, BATCH_SIZE*10)\n",
    "print(\"sigma_test = %.5e\" % sigma_test)\n",
    "print(\"E_test = %.5e\" % E_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf6Dy8NvtGDQ"
   },
   "source": [
    "#### Print out the metrics\n",
    "One can check the explicit form of the trained metrics by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['cymetric'] = mlg.loss.compute_cy_metric(K0, params0, train_set)\n",
    "test_set['cymetric'] = mlg.loss.compute_cy_metric(K0, params0, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043887055191588246\n"
     ]
    }
   ],
   "source": [
    "amp1 = mlg.loss.evaluate_dataset(K0, params0, test_set, mlg.loss.max_error, BATCH_SIZE*10)\n",
    "print(amp1)\n",
    "#amp1 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = BihomogeneousNetwork(layers=[64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = SpectralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = SpectralNetwork(layers=[64, 64, 64], amp = amp1, activation = jnp.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting K-FAC training with 100 epochs, 50 batches/epoch...\n",
      "Epoch 1: Avg Loss = 9.60293e-04\n",
      "Epoch 10: Avg Loss = 5.34137e-04\n",
      "Epoch 20: Avg Loss = 5.26789e-04\n",
      "Epoch 30: Avg Loss = 5.26925e-04\n",
      "Epoch 40: Avg Loss = 5.30205e-04\n",
      "Epoch 50: Avg Loss = 5.23385e-04\n",
      "Epoch 60: Avg Loss = 5.22396e-04\n",
      "Epoch 70: Avg Loss = 5.21767e-04\n",
      "Epoch 80: Avg Loss = 5.23250e-04\n",
      "Epoch 90: Avg Loss = 5.24254e-04\n",
      "Epoch 100: Avg Loss = 5.24159e-04\n",
      "K-FAC finished in 774.43s. Final Loss: 5.24159e-04\n"
     ]
    }
   ],
   "source": [
    "params1, final_loss1 = mlg.trainer.train_kfac(\n",
    "    model=K1,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    epochs=100,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_metric=mlg.loss.weighted_MSPE_amp_scaled,\n",
    "    residue_amp=amp1,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting training with 100 epochs, 50 batches/epoch...\n",
      "Epoch 1: Avg Loss = 8.89012e-04\n",
      "Epoch 10: Avg Loss = 5.24689e-04\n",
      "Epoch 20: Avg Loss = 5.21434e-04\n",
      "Epoch 30: Avg Loss = 5.20734e-04\n",
      "Epoch 40: Avg Loss = 5.18646e-04\n",
      "Epoch 50: Avg Loss = 5.17209e-04\n",
      "Epoch 60: Avg Loss = 5.16818e-04\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "\n",
    "params1, final_loss1 = mlg.trainer.train_optax(\n",
    "    model=K1,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    epochs=100,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_metric=mlg.loss.weighted_MSPE_amp_scaled,\n",
    "    residue_amp=amp1,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_test = mlg.loss.evaluate_dataset(K1, params1, test_set, mlg.loss.weighted_MAPE, BATCH_SIZE*10, residue_amp=amp1)\n",
    "E_test = mlg.loss.evaluate_dataset(K1, params1, test_set, mlg.loss.weighted_MSPE, BATCH_SIZE*10, residue_amp=amp1)\n",
    "print(\"sigma_test = %.5e\" % sigma_test)\n",
    "print(\"E_test = %.5e\" % E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting L-BFGS training (Accumulated Gradients)...\n",
      "Initial Loss: 0.02977\n",
      "Iteration 1: Loss = 0.02892\n",
      "Iteration 2: Loss = 0.02870\n",
      "Iteration 3: Loss = 0.02791\n",
      "Iteration 4: Loss = 0.02780\n",
      "Iteration 5: Loss = 0.02775\n",
      "Iteration 6: Loss = 0.02756\n",
      "Iteration 7: Loss = 0.02753\n",
      "Iteration 8: Loss = 0.02752\n",
      "Iteration 9: Loss = 0.02750\n",
      "Iteration 10: Loss = 0.02749\n",
      "Iteration 11: Loss = 0.02746\n",
      "Iteration 12: Loss = 0.02744\n",
      "Iteration 13: Loss = 0.02740\n",
      "Iteration 14: Loss = 0.02737\n",
      "Iteration 15: Loss = 0.02732\n",
      "Iteration 16: Loss = 0.02728\n",
      "Iteration 17: Loss = 0.02721\n",
      "Iteration 18: Loss = 0.02718\n",
      "Iteration 19: Loss = 0.02709\n",
      "Iteration 20: Loss = 0.02707\n",
      "L-BFGS finished in 295.03s. Final Loss: 0.02707\n"
     ]
    }
   ],
   "source": [
    "params2, final_loss_lbfgs = mlg.trainer.train_lbfgs(\n",
    "    model=model1,\n",
    "    params=params1, \n",
    "    dataset=train_set,\n",
    "    max_iter=20,\n",
    "    loss_metric=loss_metric,\n",
    "    batch_size=1000, # Enable gradient accumulation\n",
    "    verbose=True,\n",
    "    residue_amp=amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model1, params2, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00630686, dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['cymetric'] = train_set['cymetric'] + cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.05189428, dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.max_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model1, params2, test_set)\n",
    "metric_total = test_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = test_set['Omega_Omegabar']\n",
    "mass = test_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00715103, dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['cymetric'] = test_set['cymetric'] + cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp2 = mlg.loss.max_abs_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.08712792, dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.weighted_RMSE(omega_omegabar/amp, det_omega/amp, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01521459, dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phi2(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = bnn.Bihomogeneous(d=len(Z))(inputs)\n",
    "        x = bnn.SquareDense(features=64)(x)\n",
    "        x = bnn.SquareDense(features=128)(x)\n",
    "        x = bnn.SquareDense(features=256)(x)\n",
    "        x = bnn.SquareDense(features=1, activation=None)(x)\n",
    "        return amp2 * jnp.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = phi2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MAPE_scaled2(y_true: jnp.ndarray, y_pred: jnp.ndarray, mass: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Weighted Mean Absolute Percentage Error.\"\"\"\n",
    "    weights = mass / jnp.sum(mass)\n",
    "    return jnp.sum(jnp.abs(y_true - y_pred) / y_true * weights) / amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metric = weighted_MAPE_scaled2\n",
    "BATCH_SIZE = 1000\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting training with 300 epochs, 50 batches/epoch...\n",
      "Epoch 1: Avg Loss = 0.98891\n",
      "Epoch 10: Avg Loss = 0.42835\n",
      "Epoch 20: Avg Loss = 0.42454\n",
      "Epoch 30: Avg Loss = 0.42385\n",
      "Epoch 40: Avg Loss = 0.42337\n",
      "Epoch 50: Avg Loss = 0.42233\n",
      "Epoch 60: Avg Loss = 0.42152\n",
      "Epoch 70: Avg Loss = 0.42043\n",
      "Epoch 80: Avg Loss = 0.41826\n",
      "Epoch 90: Avg Loss = 0.41692\n",
      "Epoch 100: Avg Loss = 0.41619\n",
      "Epoch 110: Avg Loss = 0.41497\n",
      "Epoch 120: Avg Loss = 0.41476\n",
      "Epoch 130: Avg Loss = 0.41406\n",
      "Epoch 140: Avg Loss = 0.41353\n",
      "Epoch 150: Avg Loss = 0.41303\n",
      "Epoch 160: Avg Loss = 0.41316\n",
      "Epoch 170: Avg Loss = 0.41264\n",
      "Epoch 180: Avg Loss = 0.41185\n",
      "Epoch 190: Avg Loss = 0.41051\n",
      "Epoch 200: Avg Loss = 0.40874\n",
      "Epoch 210: Avg Loss = 0.40363\n",
      "Epoch 220: Avg Loss = 0.39768\n",
      "Epoch 230: Avg Loss = 0.39285\n",
      "Epoch 240: Avg Loss = 0.38566\n",
      "Epoch 250: Avg Loss = 0.37519\n",
      "Epoch 260: Avg Loss = 0.35918\n",
      "Epoch 270: Avg Loss = 0.34083\n",
      "Epoch 280: Avg Loss = 0.31817\n",
      "Epoch 290: Avg Loss = 0.29687\n",
      "Epoch 300: Avg Loss = 0.27579\n",
      "Training finished in 2269.77s. Final Loss: 0.27579\n"
     ]
    }
   ],
   "source": [
    "params3, final_loss3 = mlg.trainer.train_optax(\n",
    "    model=model2,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    optimizer=optimizer,\n",
    "    epochs=300,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_metric=loss_metric,\n",
    "    seed=10, \n",
    "    verbose=True,\n",
    "    residue_amp=amp2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params3, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00408141, dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00672522, dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.max_abs_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting L-BFGS training (Accumulated Gradients)...\n",
      "Initial Loss: 0.26826\n",
      "Iteration 1: Loss = 0.26578\n",
      "Iteration 2: Loss = 0.26464\n",
      "Iteration 3: Loss = 0.26222\n",
      "Iteration 4: Loss = 0.26063\n",
      "Iteration 5: Loss = 0.25986\n",
      "Iteration 6: Loss = 0.25916\n",
      "Iteration 7: Loss = 0.25834\n",
      "Iteration 8: Loss = 0.25752\n",
      "Iteration 9: Loss = 0.25707\n",
      "Iteration 10: Loss = 0.25665\n",
      "Iteration 11: Loss = 0.25627\n",
      "Iteration 12: Loss = 0.25595\n",
      "Iteration 13: Loss = 0.25566\n",
      "Iteration 14: Loss = 0.25539\n",
      "Iteration 15: Loss = 0.25512\n",
      "Iteration 16: Loss = 0.25487\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params4, final_loss_lbfgs = \u001b[43mmlg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enable gradient accumulation\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresidue_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamp\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/MLGeometry/src/MLGeometry/trainer.py:201\u001b[39m, in \u001b[36mtrain_lbfgs\u001b[39m\u001b[34m(model, dataset, max_iter, loss_metric, params, batch_size, residue_amp, seed, verbose, history)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_iter + \u001b[32m1\u001b[39m):\n\u001b[32m    200\u001b[39m     params, state = step(params, state)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: history.append(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/array.py:330\u001b[39m, in \u001b[36mArrayImpl.__format__\u001b[39m\u001b[34m(self, format_spec)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec):\n\u001b[32m    328\u001b[39m   \u001b[38;5;66;03m# Simulates behavior of https://github.com/numpy/numpy/pull/9883\u001b[39;00m\n\u001b[32m    329\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m[()], format_spec)\n\u001b[32m    331\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m._value, format_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/profiler.py:333\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    332\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/array.py:629\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28mself\u001b[39m._npy_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np.ndarray, \u001b[38;5;28mself\u001b[39m._npy_value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "params4, final_loss_lbfgs = mlg.trainer.train_lbfgs(\n",
    "    model=model2,\n",
    "    params=params3, \n",
    "    dataset=train_set,\n",
    "    max_iter=50,\n",
    "    loss_metric=loss_metric,\n",
    "    batch_size=1000, # Enable gradient accumulation\n",
    "    verbose=True,\n",
    "    residue_amp=amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006591698\n"
     ]
    }
   ],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params4, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007356135\n"
     ]
    }
   ],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params4, test_set)\n",
    "metric_total = test_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = test_set['Omega_Omegabar']\n",
    "mass = test_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 7.71893933e-02+3.09664756e-08j,\n",
       "         -1.67471971e-02+1.11519266e-03j,\n",
       "          2.66354643e-02+2.42956840e-02j],\n",
       "        [-1.67471562e-02-1.11527834e-03j,\n",
       "          1.16057068e-01+2.23517418e-08j,\n",
       "         -4.92643341e-02-2.43993532e-02j],\n",
       "        [ 2.66354252e-02-2.42956933e-02j,\n",
       "         -4.92642149e-02+2.43993215e-02j,\n",
       "          1.20653756e-01-9.31322575e-10j]],\n",
       "\n",
       "       [[ 6.90417737e-02+1.36205927e-08j,\n",
       "         -2.01967341e-04-2.64164968e-03j,\n",
       "         -1.89560244e-03-2.37878179e-03j],\n",
       "        [-2.01980030e-04+2.64163921e-03j,\n",
       "          7.17412829e-02-1.46683306e-08j,\n",
       "         -2.76465784e-03+3.07646522e-04j],\n",
       "        [-1.89556764e-03+2.37877155e-03j,\n",
       "         -2.76464853e-03-3.07646202e-04j,\n",
       "          6.90521747e-02+1.29221007e-08j]],\n",
       "\n",
       "       [[ 7.60340840e-02+8.73114914e-09j,\n",
       "         -5.37282787e-04-2.57899892e-03j,\n",
       "          3.47939064e-03-7.22916157e-04j],\n",
       "        [-5.37266489e-04+2.57903244e-03j,\n",
       "          9.20413807e-02+2.20024958e-08j,\n",
       "          3.23314033e-03-9.30126640e-04j],\n",
       "        [ 3.47940763e-03+7.22914934e-04j,\n",
       "          3.23316897e-03+9.30120237e-04j,\n",
       "          6.43355176e-02+9.53150447e-10j]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.72815964e-02+4.19095159e-09j,\n",
       "         -1.60954846e-03-3.25829163e-03j,\n",
       "          1.60539895e-02-8.19160789e-03j],\n",
       "        [-1.60953857e-03+3.25828837e-03j,\n",
       "          6.51455596e-02-3.73984221e-09j,\n",
       "          4.30789171e-03-7.27851130e-03j],\n",
       "        [ 1.60539541e-02+8.19158740e-03j,\n",
       "          4.30784468e-03+7.27848057e-03j,\n",
       "          1.09562896e-01-7.82310963e-08j]],\n",
       "\n",
       "       [[ 8.03252608e-02+4.00468707e-08j,\n",
       "         -4.45630495e-03-1.07107572e-02j,\n",
       "          1.78992897e-02-1.25623802e-02j],\n",
       "        [-4.45629051e-03+1.07107311e-02j,\n",
       "          6.10619858e-02-1.74622983e-09j,\n",
       "         -4.86414996e-04+4.38420381e-03j],\n",
       "        [ 1.78992637e-02+1.25623588e-02j,\n",
       "         -4.86419653e-04-4.38420987e-03j,\n",
       "          8.66505951e-02-5.82076609e-09j]],\n",
       "\n",
       "       [[ 8.75796899e-02-1.35041773e-08j,\n",
       "          1.74076320e-03-9.85580264e-04j,\n",
       "          1.40405400e-02+2.62528136e-02j],\n",
       "        [ 1.74074830e-03+9.85600520e-04j,\n",
       "          6.72407597e-02+1.92085281e-08j,\n",
       "          1.62283191e-03+1.25227626e-02j],\n",
       "        [ 1.40405698e-02-2.62527820e-02j,\n",
       "          1.62286800e-03-1.25227086e-02j,\n",
       "          2.15554401e-01+4.47034836e-08j]]], dtype=complex64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.8288648],\n",
       "       [-5.2468724],\n",
       "       [-4.2695985],\n",
       "       ...,\n",
       "       [-2.7088165],\n",
       "       [-2.788074 ],\n",
       "       [-4.42008  ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.apply(params4, test_set['points']) / amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01521459, dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:JAX]",
   "language": "python",
   "name": "conda-env-JAX-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
