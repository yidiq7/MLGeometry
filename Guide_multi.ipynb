{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FIm6mUYtGC5"
   },
   "source": [
    "# MLGeometry guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCemOECptGC6"
   },
   "source": [
    "This introduction demonstrates how to use MLGeometry to:\n",
    "1. Generate a hypersurface.\n",
    "2. Build a bihomogeneous neural network.\n",
    "3. Use the model to compute numerical Calabi-Yau metrics with the embedding method.\n",
    "4. Plot $\\eta$ on a rational curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilHaPYnkEi-S"
   },
   "source": [
    "## Install the package (on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install MLGeometry-JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyFvWKNmtGC7"
   },
   "source": [
    "## Configure imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "doBhWopntGC9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import pickle\n",
    "\n",
    "import MLGeometry as mlg\n",
    "from MLGeometry import bihomoNN as bnn\n",
    "from typing import Sequence, Any, List, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9F3AKqPtGC_"
   },
   "source": [
    "Import the libraries to plot the $\\eta$ on the rational curve (see the last section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J9LWSTH3tGC_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLGeometry: Precision set to FP64 (x64 enabled).\n"
     ]
    }
   ],
   "source": [
    "mlg.set_precision(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd1zG07TtGDA"
   },
   "source": [
    "## Set a random seed (optional)\n",
    "Some random seed might be bad for numerical calulations. If there are any errors during the training, you may want to try a different seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "x5Rz0lXmtGDB"
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "rng = jax.random.PRNGKey(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjB84Ln1tGDB"
   },
   "source": [
    "## Define a hypersurface\n",
    "First define a set of coordinates and a function as sympy symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "pwhQynDBtGDB"
   },
   "outputs": [],
   "source": [
    "z0, z1, z2, z3, z4 = sp.symbols('z0, z1, z2, z3, z4')\n",
    "Z = [z0,z1,z2,z3,z4]\n",
    "f = z0**5 + z1**5 + z2**5 + z3**5 + z4**5 + 0.5*z0*z1*z2*z3*z4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTm215kftGDC"
   },
   "source": [
    "Then define a hypersurface as a collection of points which solve the equation f = 0, using the `Hypersurface` class in the `mlg.hypersurface` module. The parameter n_pairs is the number of random pairs of points used to form the random lines in $\\mathbf{CP}^{N+1}$. Then we take the intersections of those random lines and the hypersurface. By Bezout's theorem, each line intersects the hypersurface in precisely d points where d is the number of homogeneous coordinates. So the total number of points is d * n_pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "jz1Vi4Y2tGDC"
   },
   "outputs": [],
   "source": [
    "n_pairs = 10000\n",
    "HS_train = mlg.hypersurface.Hypersurface(Z, f, n_pairs)\n",
    "HS_test = mlg.hypersurface.Hypersurface(Z, f, n_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE981r2ctGDC"
   },
   "source": [
    "The Hypersurface class will take care of the patchwork automatically. Let's use the `list_patches` function to check the number of points on each patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zyw84dftGDC",
    "outputId": "e8fb2f30-eadf-4129-80c6-33971a79b273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patches: 5\n",
      "Points on patch 1 : 10171\n",
      "Points on patch 2 : 10038\n",
      "Points on patch 3 : 9783\n",
      "Points on patch 4 : 10047\n",
      "Points on patch 5 : 9961\n"
     ]
    }
   ],
   "source": [
    "HS_train.list_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JtfwpmFtGDD"
   },
   "source": [
    "You can also invoke this method on one of the patches to check the distribution on the subpatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fMZAyKutGDD",
    "outputId": "5b432e0a-36e6-4ac5-9dda-4c9447aa656a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patches: 4\n",
      "Points on patch 1 : 2588\n",
      "Points on patch 2 : 2530\n",
      "Points on patch 3 : 2512\n",
      "Points on patch 4 : 2541\n"
     ]
    }
   ],
   "source": [
    "HS_train.patches[0].list_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8p5D9ZWtGDE"
   },
   "source": [
    "The Hypersurface class contains some symbolic and numerical methods as well, which will be introduced elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u1oIOLytGDE"
   },
   "source": [
    "## Training with JAX\n",
    "\n",
    "### Generate datasets\n",
    "The `mlg.dataset.generate_dataset` function converts a hypersurface to a dataset dictionary, which has four componets: the points on the hypersurface, the volume form $\\small \\Omega \\wedge \\bar\\Omega$, the mass reweighting the points distribution and the restriction which restricts the Kähler metric to a subpatch. The restriction contains an extra linear transformation so that points on different affine patches can all be processed in one call. It is also possible to generate a dataset only on one affine patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "uGq-mKdDtGDE"
   },
   "outputs": [],
   "source": [
    "train_set = mlg.dataset.generate_dataset(HS_train)\n",
    "test_set = mlg.dataset.generate_dataset(HS_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHP3ExA1tGDG"
   },
   "source": [
    "Shuffle and batch the datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7Mga5fdtGDG"
   },
   "source": [
    "Let's look at what is inside a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  points: shape (50000, 5), dtype complex64\n",
      "  Omega_Omegabar: shape (50000,), dtype float32\n",
      "  mass: shape (50000,), dtype float32\n",
      "  restriction: shape (50000, 3, 5), dtype complex64\n"
     ]
    }
   ],
   "source": [
    "for key, val in train_set.items():\n",
    "    print(f\"  {key}: shape {val.shape}, dtype {val.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kVSGnnktGDH"
   },
   "source": [
    "### Build a bihomogeneous neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV_JRrERtGDI"
   },
   "source": [
    "The `mlg.bihomoNN` module provides the necessary layers (e.g. `Bihomogeneous` and `SquareDense` ) to construct the Kähler potential with a bihomogeneous neural network. Here is an example of a two-hidden-layer network (k = 4) with 70 and 100 hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "B9mE_YLltGDI"
   },
   "outputs": [],
   "source": [
    "class BihomogeneousNetwork(nn.Module):\n",
    "    layers: List[int]\n",
    "    amp: Any = 1.0\n",
    "    d: int = 5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = bnn.Bihomogeneous(d=self.d)(inputs)\n",
    "        for feat in self.layers:\n",
    "            x = bnn.SquareDense(features=feat)(x)\n",
    "        x = bnn.SquareDense(features=1, activation=None)(x)\n",
    "\n",
    "        return self.amp * jnp.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNetwork(nn.Module):\n",
    "    layers: List[int]\n",
    "    amp: Any = 1.0\n",
    "    activation: Callable = nn.tanh\n",
    "    d: int = 5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = bnn.Spectral(d=self.d)(inputs)\n",
    "        for feat in self.layers:\n",
    "            x = nn.Dense(features=feat)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(features=1)(x)\n",
    "\n",
    "        return self.amp * jnp.sinh(x)\n",
    "        #return self.amp * jnp.sign(x) * (jnp.exp(jnp.abs(x)) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "cXSQuJgCtGDI"
   },
   "outputs": [],
   "source": [
    "K0 = BihomogeneousNetwork(layers=[64, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSvy2QCmtGDJ"
   },
   "source": [
    "### Train the model with Adam and L-BFGS\n",
    "#### Adam\n",
    "Setup the keras optmizer as `Adam` and the loss function as one of weighted loss in the `mlg.loss` module. Some available functions are `weighted_MAPE`, `weighted_MSPE`, `max_error` and `MAPE_plus_max_error`. They are weighted with the mass formula since the points on the hypersurface are distributed according to the Fubini-Study measure while the measure used in the integration is determined by the volume form $\\small \\Omega \\wedge \\bar\\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting K-FAC training with 50 epochs, 50 batches/epoch...\n",
      "Epoch 1: Avg Loss = 0.22872\n",
      "Epoch 10: Avg Loss = 0.00078\n",
      "Epoch 20: Avg Loss = 0.00043\n",
      "Epoch 30: Avg Loss = 0.00034\n",
      "Epoch 40: Avg Loss = 0.00030\n",
      "Epoch 50: Avg Loss = 0.00027\n",
      "K-FAC finished in 807.78s. Final Loss: 0.00027\n"
     ]
    }
   ],
   "source": [
    "loss_metric = mlg.loss.weighted_MSPE\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "params, final_loss = mlg.trainer.train_kfac(\n",
    "    model=K0,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_metric=loss_metric,\n",
    "    seed=7,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8R-_rO7tGDK"
   },
   "source": [
    "Loop over the batches and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjRWkGZ4tGDQ",
    "outputId": "273e4c93-53d9-4783-84f0-3e940aab0bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_test = 0.01288\n",
      "E_test = 0.00028\n"
     ]
    }
   ],
   "source": [
    "sigma_test = mlg.loss.evaluate_dataset(K0, params, test_set, mlg.loss.weighted_MAPE, BATCH_SIZE*10)\n",
    "E_test = mlg.loss.evaluate_dataset(K0, params, test_set, mlg.loss.weighted_MSPE, BATCH_SIZE*10)\n",
    "print(\"sigma_test = %.5f\" % sigma_test)\n",
    "print(\"E_test = %.5f\" % E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'trained_models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model_path = os.path.join(output_dir, '64_128_1.pkl')\n",
    "    \n",
    "with open(model_path, 'wb') as f_pkl:\n",
    "    pickle.dump(params, f_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf6Dy8NvtGDQ"
   },
   "source": [
    "#### Print out the metrics\n",
    "One can check the explicit form of the trained metrics by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['cymetric'] = mlg.loss.compute_cy_metric(K0, params, train_set)\n",
    "test_set['cymetric'] = mlg.loss.compute_cy_metric(K0, params, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23411956007953844\n"
     ]
    }
   ],
   "source": [
    "amp1 = mlg.loss.evaluate_dataset(K0, params, test_set, mlg.loss.max_error, BATCH_SIZE*10)\n",
    "print(amp1)\n",
    "amp1 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = BihomogeneousNetwork(layers=[64, 128, 256], amp = amp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = SpectralNetwork(layers=[64, 64, 64], amp = amp1, activation = nn.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "jax.config.update('jax_disable_jit', False)\n",
    "config.update(\"jax_debug_nans\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting K-FAC training with 100 epochs, 500 batches/epoch...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params1, final_loss1 = \u001b[43mmlg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_kfac\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweighted_MSPE_amp_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresidue_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/MLGeometry/src/MLGeometry/trainer.py:261\u001b[39m, in \u001b[36mtrain_kfac\u001b[39m\u001b[34m(model, dataset, epochs, batch_size, loss_metric, params, residue_amp, seed, verbose, history)\u001b[39m\n\u001b[32m    258\u001b[39m rng, step_rng = jax.random.split(rng)\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Step without top-level JIT to allow K-FAC internal control flow\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m params, opt_state, stats = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_step\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m epoch_loss += stats[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    266\u001b[39m global_step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/optimizer.py:1336\u001b[39m, in \u001b[36mOptimizer.step\u001b[39m\u001b[34m(self, params, state, rng, data_iterator, batch, func_state, learning_rate, momentum, damping, global_step_int)\u001b[39m\n\u001b[32m   1331\u001b[39m should_update_estimate_curvature = \u001b[38;5;28mself\u001b[39m.should_update_estimate_curvature(\n\u001b[32m   1332\u001b[39m     step_counter_int\n\u001b[32m   1333\u001b[39m )\n\u001b[32m   1334\u001b[39m should_update_damping = \u001b[38;5;28mself\u001b[39m.should_update_damping(step_counter_int)\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshould_update_estimate_curvature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_update_damping\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/utils/staging.py:300\u001b[39m, in \u001b[36mstaged.<locals>.decorated\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m     outs = func(instance, *args, **kwargs)\n\u001b[32m    299\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     outs = \u001b[43mjitted_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "    \u001b[31m[... skipping hidden 15 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/utils/misc.py:342\u001b[39m, in \u001b[36mauto_scope_method.<locals>.wrapped\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m   method_name = method_name[\u001b[32m1\u001b[39m:]\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.named_scope(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/optimizer.py:1110\u001b[39m, in \u001b[36mOptimizer._step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1107\u001b[39m state = \u001b[38;5;28mself\u001b[39m._maybe_update_inverse_cache(state, damping)\n\u001b[32m   1109\u001b[39m \u001b[38;5;66;03m# Compute proposed directions\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m preconditioned_gradient = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_preconditioned_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamping\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# constrain the norms\u001b[39;00m\n\u001b[32m   1115\u001b[39m preconditioned_gradient, scaled_grad_norm_sq = (\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_apply_norm_constraint(\n\u001b[32m   1117\u001b[39m         grads, preconditioned_gradient, learning_rate,\n\u001b[32m   1118\u001b[39m     )\n\u001b[32m   1119\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/utils/staging.py:233\u001b[39m, in \u001b[36mstaged.<locals>.decorated\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    230\u001b[39m args, kwargs = bound_args.args[\u001b[32m1\u001b[39m:], bound_args.kwargs\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance.in_staging:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m instance.staging_context():\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m instance.multi_device \u001b[38;5;129;01mand\u001b[39;00m instance.debug:\n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# In this case we want to call `method` once for each device index.\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Note that this might not always produce sensible behavior, and will\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# depend on the details of the method and if it has side effects on the\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# state of the class. Note that pmean operations won't happen, since the\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m# actual output of pmapped methods won't be numerically correct.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/optimizer.py:812\u001b[39m, in \u001b[36mOptimizer._compute_preconditioned_gradient\u001b[39m\u001b[34m(self, state, grads, damping)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;129m@utils\u001b[39m.staged\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compute_preconditioned_gradient\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    808\u001b[39m     damping: Array,\n\u001b[32m    809\u001b[39m ) -> Params:\n\u001b[32m    810\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Computes the preconditioned gradient.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m812\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultiply_matpower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m      \u001b[49m\u001b[43mparameter_structured_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m      \u001b[49m\u001b[43midentity_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_precon_damping_mult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpower\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_precon_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexact_power\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_use_exact_inverses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m      \u001b[49m\u001b[43muse_cached\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_use_cached_inverses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpmap_axis_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpmap_axis_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnorm_to_scale_identity_weight_per_block\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_norm_to_scale_identity_weight_per_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/utils/misc.py:342\u001b[39m, in \u001b[36mauto_scope_method.<locals>.wrapped\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m   method_name = method_name[\u001b[32m1\u001b[39m:]\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.named_scope(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/kfac_jax/_src/curvature_estimator/block_diagonal.py:500\u001b[39m, in \u001b[36mBlockDiagonalCurvature.multiply_matpower\u001b[39m\u001b[34m(self, state, parameter_structured_vector, identity_weight, power, exact_power, use_cached, pmap_axis_name, norm_to_scale_identity_weight_per_block)\u001b[39m\n\u001b[32m    496\u001b[39m   result = \u001b[38;5;28mtuple\u001b[39m(thunk() \u001b[38;5;28;01mfor\u001b[39;00m thunk \u001b[38;5;129;01min\u001b[39;00m thunks)\n\u001b[32m    498\u001b[39m parameter_structured_result = \u001b[38;5;28mself\u001b[39m.blocks_vectors_to_params_vector(result)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m utils.abstract_objects_equal(\n\u001b[32m    501\u001b[39m     parameter_structured_vector, parameter_structured_result)\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m parameter_structured_result\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "params1, final_loss1 = mlg.trainer.train_kfac(\n",
    "    model=K1,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    epochs=100,\n",
    "    batch_size=100,\n",
    "    loss_metric=mlg.loss.weighted_MSPE_amp_scaled,\n",
    "    residue_amp=amp1,\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_test = mlg.loss.evaluate_dataset(K1, params1, test_set, mlg.loss.weighted_MAPE, BATCH_SIZE*10, residue_amp=amp1)\n",
    "E_test = mlg.loss.evaluate_dataset(K1, params1, test_set, mlg.loss.weighted_MSPE, BATCH_SIZE*10, residue_amp=amp1)\n",
    "print(\"sigma_test = %.8f\" % sigma_test)\n",
    "print(\"E_test = %.8f\" % E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting L-BFGS training (Accumulated Gradients)...\n",
      "Initial Loss: 0.02977\n",
      "Iteration 1: Loss = 0.02892\n",
      "Iteration 2: Loss = 0.02870\n",
      "Iteration 3: Loss = 0.02791\n",
      "Iteration 4: Loss = 0.02780\n",
      "Iteration 5: Loss = 0.02775\n",
      "Iteration 6: Loss = 0.02756\n",
      "Iteration 7: Loss = 0.02753\n",
      "Iteration 8: Loss = 0.02752\n",
      "Iteration 9: Loss = 0.02750\n",
      "Iteration 10: Loss = 0.02749\n",
      "Iteration 11: Loss = 0.02746\n",
      "Iteration 12: Loss = 0.02744\n",
      "Iteration 13: Loss = 0.02740\n",
      "Iteration 14: Loss = 0.02737\n",
      "Iteration 15: Loss = 0.02732\n",
      "Iteration 16: Loss = 0.02728\n",
      "Iteration 17: Loss = 0.02721\n",
      "Iteration 18: Loss = 0.02718\n",
      "Iteration 19: Loss = 0.02709\n",
      "Iteration 20: Loss = 0.02707\n",
      "L-BFGS finished in 295.03s. Final Loss: 0.02707\n"
     ]
    }
   ],
   "source": [
    "params2, final_loss_lbfgs = mlg.trainer.train_lbfgs(\n",
    "    model=model1,\n",
    "    params=params1, \n",
    "    dataset=train_set,\n",
    "    max_iter=20,\n",
    "    loss_metric=loss_metric,\n",
    "    batch_size=1000, # Enable gradient accumulation\n",
    "    verbose=True,\n",
    "    residue_amp=amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model1, params2, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00630686, dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['cymetric'] = train_set['cymetric'] + cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.05189428, dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.max_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model1, params2, test_set)\n",
    "metric_total = test_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = test_set['Omega_Omegabar']\n",
    "mass = test_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00715103, dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['cymetric'] = test_set['cymetric'] + cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp2 = mlg.loss.max_abs_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.08712792, dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.weighted_RMSE(omega_omegabar/amp, det_omega/amp, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01521459, dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phi2(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = bnn.Bihomogeneous(d=len(Z))(inputs)\n",
    "        x = bnn.SquareDense(features=64)(x)\n",
    "        x = bnn.SquareDense(features=128)(x)\n",
    "        x = bnn.SquareDense(features=256)(x)\n",
    "        x = bnn.SquareDense(features=1, activation=None)(x)\n",
    "        return amp2 * jnp.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = phi2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_MAPE_scaled2(y_true: jnp.ndarray, y_pred: jnp.ndarray, mass: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Weighted Mean Absolute Percentage Error.\"\"\"\n",
    "    weights = mass / jnp.sum(mass)\n",
    "    return jnp.sum(jnp.abs(y_true - y_pred) / y_true * weights) / amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metric = weighted_MAPE_scaled2\n",
    "BATCH_SIZE = 1000\n",
    "optimizer = optax.adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters for input dimension 5...\n",
      "Starting training with 300 epochs, 50 batches/epoch...\n",
      "Epoch 1: Avg Loss = 0.98891\n",
      "Epoch 10: Avg Loss = 0.42835\n",
      "Epoch 20: Avg Loss = 0.42454\n",
      "Epoch 30: Avg Loss = 0.42385\n",
      "Epoch 40: Avg Loss = 0.42337\n",
      "Epoch 50: Avg Loss = 0.42233\n",
      "Epoch 60: Avg Loss = 0.42152\n",
      "Epoch 70: Avg Loss = 0.42043\n",
      "Epoch 80: Avg Loss = 0.41826\n",
      "Epoch 90: Avg Loss = 0.41692\n",
      "Epoch 100: Avg Loss = 0.41619\n",
      "Epoch 110: Avg Loss = 0.41497\n",
      "Epoch 120: Avg Loss = 0.41476\n",
      "Epoch 130: Avg Loss = 0.41406\n",
      "Epoch 140: Avg Loss = 0.41353\n",
      "Epoch 150: Avg Loss = 0.41303\n",
      "Epoch 160: Avg Loss = 0.41316\n",
      "Epoch 170: Avg Loss = 0.41264\n",
      "Epoch 180: Avg Loss = 0.41185\n",
      "Epoch 190: Avg Loss = 0.41051\n",
      "Epoch 200: Avg Loss = 0.40874\n",
      "Epoch 210: Avg Loss = 0.40363\n",
      "Epoch 220: Avg Loss = 0.39768\n",
      "Epoch 230: Avg Loss = 0.39285\n",
      "Epoch 240: Avg Loss = 0.38566\n",
      "Epoch 250: Avg Loss = 0.37519\n",
      "Epoch 260: Avg Loss = 0.35918\n",
      "Epoch 270: Avg Loss = 0.34083\n",
      "Epoch 280: Avg Loss = 0.31817\n",
      "Epoch 290: Avg Loss = 0.29687\n",
      "Epoch 300: Avg Loss = 0.27579\n",
      "Training finished in 2269.77s. Final Loss: 0.27579\n"
     ]
    }
   ],
   "source": [
    "params3, final_loss3 = mlg.trainer.train_optax(\n",
    "    model=model2,\n",
    "    params=None, \n",
    "    dataset=train_set,\n",
    "    optimizer=optimizer,\n",
    "    epochs=300,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_metric=loss_metric,\n",
    "    seed=10, \n",
    "    verbose=True,\n",
    "    residue_amp=amp2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params3, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00408141, dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00672522, dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlg.loss.max_abs_error(omega_omegabar, det_omega, mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting L-BFGS training (Accumulated Gradients)...\n",
      "Initial Loss: 0.26826\n",
      "Iteration 1: Loss = 0.26578\n",
      "Iteration 2: Loss = 0.26464\n",
      "Iteration 3: Loss = 0.26222\n",
      "Iteration 4: Loss = 0.26063\n",
      "Iteration 5: Loss = 0.25986\n",
      "Iteration 6: Loss = 0.25916\n",
      "Iteration 7: Loss = 0.25834\n",
      "Iteration 8: Loss = 0.25752\n",
      "Iteration 9: Loss = 0.25707\n",
      "Iteration 10: Loss = 0.25665\n",
      "Iteration 11: Loss = 0.25627\n",
      "Iteration 12: Loss = 0.25595\n",
      "Iteration 13: Loss = 0.25566\n",
      "Iteration 14: Loss = 0.25539\n",
      "Iteration 15: Loss = 0.25512\n",
      "Iteration 16: Loss = 0.25487\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[107]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params4, final_loss_lbfgs = \u001b[43mmlg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Enable gradient accumulation\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresidue_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamp\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/MLGeometry/src/MLGeometry/trainer.py:201\u001b[39m, in \u001b[36mtrain_lbfgs\u001b[39m\u001b[34m(model, dataset, max_iter, loss_metric, params, batch_size, residue_amp, seed, verbose, history)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_iter + \u001b[32m1\u001b[39m):\n\u001b[32m    200\u001b[39m     params, state = step(params, state)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.value\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: history.append(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/array.py:330\u001b[39m, in \u001b[36mArrayImpl.__format__\u001b[39m\u001b[34m(self, format_spec)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec):\n\u001b[32m    328\u001b[39m   \u001b[38;5;66;03m# Simulates behavior of https://github.com/numpy/numpy/pull/9883\u001b[39;00m\n\u001b[32m    329\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m[()], format_spec)\n\u001b[32m    331\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m._value, format_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/profiler.py:333\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    332\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/JAX/lib/python3.11/site-packages/jax/_src/array.py:629\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28mself\u001b[39m._npy_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np.ndarray, \u001b[38;5;28mself\u001b[39m._npy_value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "params4, final_loss_lbfgs = mlg.trainer.train_lbfgs(\n",
    "    model=model2,\n",
    "    params=params3, \n",
    "    dataset=train_set,\n",
    "    max_iter=50,\n",
    "    loss_metric=loss_metric,\n",
    "    batch_size=1000, # Enable gradient accumulation\n",
    "    verbose=True,\n",
    "    residue_amp=amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006591698\n"
     ]
    }
   ],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params4, train_set)\n",
    "metric_total = train_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = train_set['Omega_Omegabar']\n",
    "mass = train_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007356135\n"
     ]
    }
   ],
   "source": [
    "cymetric_delta = mlg.loss.compute_cy_metric(model2, params4, test_set)\n",
    "metric_total = test_set['cymetric'] + cymetric_delta\n",
    "\n",
    "det_vol = jnp.real(jax.vmap(jnp.linalg.det)(metric_total))\n",
    "\n",
    "omega_omegabar = test_set['Omega_Omegabar']\n",
    "mass = test_set['mass']\n",
    "weights = mass / jnp.sum(mass)\n",
    "factor = jnp.sum(weights*det_vol/omega_omegabar)\n",
    "det_omega = det_vol / factor\n",
    "\n",
    "loss = mlg.loss.weighted_MAPE(omega_omegabar, det_omega, mass)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 7.71893933e-02+3.09664756e-08j,\n",
       "         -1.67471971e-02+1.11519266e-03j,\n",
       "          2.66354643e-02+2.42956840e-02j],\n",
       "        [-1.67471562e-02-1.11527834e-03j,\n",
       "          1.16057068e-01+2.23517418e-08j,\n",
       "         -4.92643341e-02-2.43993532e-02j],\n",
       "        [ 2.66354252e-02-2.42956933e-02j,\n",
       "         -4.92642149e-02+2.43993215e-02j,\n",
       "          1.20653756e-01-9.31322575e-10j]],\n",
       "\n",
       "       [[ 6.90417737e-02+1.36205927e-08j,\n",
       "         -2.01967341e-04-2.64164968e-03j,\n",
       "         -1.89560244e-03-2.37878179e-03j],\n",
       "        [-2.01980030e-04+2.64163921e-03j,\n",
       "          7.17412829e-02-1.46683306e-08j,\n",
       "         -2.76465784e-03+3.07646522e-04j],\n",
       "        [-1.89556764e-03+2.37877155e-03j,\n",
       "         -2.76464853e-03-3.07646202e-04j,\n",
       "          6.90521747e-02+1.29221007e-08j]],\n",
       "\n",
       "       [[ 7.60340840e-02+8.73114914e-09j,\n",
       "         -5.37282787e-04-2.57899892e-03j,\n",
       "          3.47939064e-03-7.22916157e-04j],\n",
       "        [-5.37266489e-04+2.57903244e-03j,\n",
       "          9.20413807e-02+2.20024958e-08j,\n",
       "          3.23314033e-03-9.30126640e-04j],\n",
       "        [ 3.47940763e-03+7.22914934e-04j,\n",
       "          3.23316897e-03+9.30120237e-04j,\n",
       "          6.43355176e-02+9.53150447e-10j]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.72815964e-02+4.19095159e-09j,\n",
       "         -1.60954846e-03-3.25829163e-03j,\n",
       "          1.60539895e-02-8.19160789e-03j],\n",
       "        [-1.60953857e-03+3.25828837e-03j,\n",
       "          6.51455596e-02-3.73984221e-09j,\n",
       "          4.30789171e-03-7.27851130e-03j],\n",
       "        [ 1.60539541e-02+8.19158740e-03j,\n",
       "          4.30784468e-03+7.27848057e-03j,\n",
       "          1.09562896e-01-7.82310963e-08j]],\n",
       "\n",
       "       [[ 8.03252608e-02+4.00468707e-08j,\n",
       "         -4.45630495e-03-1.07107572e-02j,\n",
       "          1.78992897e-02-1.25623802e-02j],\n",
       "        [-4.45629051e-03+1.07107311e-02j,\n",
       "          6.10619858e-02-1.74622983e-09j,\n",
       "         -4.86414996e-04+4.38420381e-03j],\n",
       "        [ 1.78992637e-02+1.25623588e-02j,\n",
       "         -4.86419653e-04-4.38420987e-03j,\n",
       "          8.66505951e-02-5.82076609e-09j]],\n",
       "\n",
       "       [[ 8.75796899e-02-1.35041773e-08j,\n",
       "          1.74076320e-03-9.85580264e-04j,\n",
       "          1.40405400e-02+2.62528136e-02j],\n",
       "        [ 1.74074830e-03+9.85600520e-04j,\n",
       "          6.72407597e-02+1.92085281e-08j,\n",
       "          1.62283191e-03+1.25227626e-02j],\n",
       "        [ 1.40405698e-02-2.62527820e-02j,\n",
       "          1.62286800e-03-1.25227086e-02j,\n",
       "          2.15554401e-01+4.47034836e-08j]]], dtype=complex64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cymetric_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.8288648],\n",
       "       [-5.2468724],\n",
       "       [-4.2695985],\n",
       "       ...,\n",
       "       [-2.7088165],\n",
       "       [-2.788074 ],\n",
       "       [-4.42008  ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.apply(params4, test_set['points']) / amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.01521459, dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:JAX]",
   "language": "python",
   "name": "conda-env-JAX-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
